{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaveeshaChan/SDGP-SE-37/blob/main/Hood_damage_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-ludeEHr6Y3",
        "outputId": "f0735de8-0641-4fe8-9a2f-d2e3cb8460fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORYP8Yofmp_7"
      },
      "source": [
        "Image, Rename and Convert to png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzqxy2iHnQa_"
      },
      "source": [
        "Convert to PNG and rename file for Hood Undamaged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djYTEWETnTuf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# set paths\n",
        "input_folder = \"/content/drive/MyDrive/RenamedDataset2/Renamed/not damaged/hood\"\n",
        "output_folder = \"/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/undamaged_hood\"\n",
        "\n",
        "# create output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# iterate through all files in input folder\n",
        "for i, filename in enumerate(os.listdir(input_folder)):\n",
        "    # only process image files\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
        "        # open the image file\n",
        "        with Image.open(os.path.join(input_folder, filename)) as img:\n",
        "            # convert to PNG format\n",
        "            img = img.convert(\"RGB\")\n",
        "            # save with new filename and extension\n",
        "            new_filename = \"Hood_\"+ str(i+1) + \".png\"\n",
        "            img.save(os.path.join(output_folder, new_filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S9tYm0MmhNx"
      },
      "source": [
        "Convert to PNG and rename file for Hood(minor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dD6kbHVm9zi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# set paths\n",
        "input_folder = \"/content/drive/MyDrive/RenamedDataset2/Renamed/damged/Minor Damages/hood(minor)\"\n",
        "output_folder = \"/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/hood(minor)\"\n",
        "\n",
        "# create output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# iterate through all files in input folder\n",
        "for i, filename in enumerate(os.listdir(input_folder)):\n",
        "    # only process image files\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
        "        # open the image file\n",
        "        with Image.open(os.path.join(input_folder, filename)) as img:\n",
        "            # convert to PNG format\n",
        "            img = img.convert(\"RGB\")\n",
        "            # save with new filename and extension\n",
        "            new_filename = \"Hood(minor)_\"+ str(i+1) + \".png\"\n",
        "            img.save(os.path.join(output_folder, new_filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkK6vGlKmcjP"
      },
      "source": [
        "Convert to PNG and rename file for Hood(moderate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDaHGuQQnKH1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# set paths\n",
        "input_folder = \"/content/drive/MyDrive/RenamedDataset2/Renamed/damged/Moderate Damages/hood (moderate)\"\n",
        "output_folder = \"/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/hood (moderate)\"\n",
        "\n",
        "# create output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# iterate through all files in input folder\n",
        "for i, filename in enumerate(os.listdir(input_folder)):\n",
        "    # only process image files\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
        "        # open the image file\n",
        "        with Image.open(os.path.join(input_folder, filename)) as img:\n",
        "            # convert to PNG format\n",
        "            img = img.convert(\"RGB\")\n",
        "            # save with new filename and extension\n",
        "            new_filename = \"Hood(moderate)_\"+ str(i+1) + \".png\"\n",
        "            img.save(os.path.join(output_folder, new_filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-261z2_lq9t"
      },
      "source": [
        "Convert to PNG and rename file for Hood(severe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "071v_zYHr71V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# set paths\n",
        "input_folder = \"/content/drive/MyDrive/RenamedDataset2/Renamed/damged/Severe Damages/hood (severe)\"\n",
        "output_folder = \"/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/hood (severe)\"\n",
        "\n",
        "# create output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# iterate through all files in input folder\n",
        "for i, filename in enumerate(os.listdir(input_folder)):\n",
        "    # only process image files\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
        "        # open the image file\n",
        "        with Image.open(os.path.join(input_folder, filename)) as img:\n",
        "            # convert to PNG format\n",
        "            img = img.convert(\"RGB\")\n",
        "            # save with new filename and extension\n",
        "            new_filename = \"Hood(severe)_\"+ str(i+1) + \".png\"\n",
        "            img.save(os.path.join(output_folder, new_filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGulSvGNm1tq",
        "outputId": "8473e242-ef6b-4ec7-d702-080a7f55b6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subfolder: hood (severe) Image count: 50\n",
            "Subfolder: hood (moderate) Image count: 83\n",
            "Subfolder: undamaged_hood Image count: 64\n",
            "Subfolder: hood(minor) Image count: 75\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2\"\n",
        "subdirs = os.listdir(data_dir)\n",
        "\n",
        "for subdir in subdirs:\n",
        "    subdir_path = os.path.join(data_dir, subdir)\n",
        "    if os.path.isdir(subdir_path):\n",
        "        count = len(os.listdir(subdir_path))\n",
        "        print(\"Subfolder:\", subdir, \"Image count:\", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuvImUG_tEu0",
        "outputId": "abe201a5-56a8-4cbd-9cca-0073a03ba902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-lrzacjra\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-lrzacjra\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (67.7.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0) (1.16.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp310-cp310-linux_x86_64.whl size=375644 sha256=705130cb4f4386ab0c34f0794311f4577ace5b45b70d1b310892dda71691f055\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-esjajo63/wheels/39/61/b4/480fbddb4d3d6bc34083e7397bc6f5d1381f79acc68e9f3511\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.7\n",
            "    Uninstalling pycocotools-2.0.7:\n",
            "      Successfully uninstalled pycocotools-2.0.7\n",
            "Successfully installed pycocotools-2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR5IT93CtSGt",
        "outputId": "7cc79545-cb33-4494-dc15-5aca7e68a112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-object-detection-api\n",
            "  Downloading tensorflow_object_detection_api-0.1.1.tar.gz (577 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/577.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/577.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m450.6/577.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.4/577.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-object-detection-api) (9.4.0)\n",
            "Requirement already satisfied: Matplotlib>=2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-object-detection-api) (3.7.1)\n",
            "Requirement already satisfied: Cython>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-object-detection-api) (3.0.8)\n",
            "Requirement already satisfied: Protobuf in /usr/local/lib/python3.10/dist-packages (from tensorflow-object-detection-api) (3.20.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from tensorflow-object-detection-api) (4.9.4)\n",
            "Collecting jupyter (from tensorflow-object-detection-api)\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from tensorflow-object-detection-api) (2.15.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-object-detection-api) (21.6.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-object-detection-api) (0.42.0)\n",
            "Collecting twine (from tensorflow-object-detection-api)\n",
            "  Downloading twine-5.0.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (2.8.2)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->tensorflow-object-detection-api) (6.5.5)\n",
            "Collecting qtconsole (from jupyter->tensorflow-object-detection-api)\n",
            "  Downloading qtconsole-5.5.1-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->tensorflow-object-detection-api) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->tensorflow-object-detection-api) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->tensorflow-object-detection-api) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->tensorflow-object-detection-api) (7.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.15.0)\n",
            "Collecting pkginfo>=1.8.1 (from twine->tensorflow-object-detection-api)\n",
            "  Downloading pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
            "Collecting readme-renderer>=35.0 (from twine->tensorflow-object-detection-api)\n",
            "  Downloading readme_renderer-43.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from twine->tensorflow-object-detection-api) (2.31.0)\n",
            "Collecting requests-toolbelt!=0.9.0,>=0.8.0 (from twine->tensorflow-object-detection-api)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from twine->tensorflow-object-detection-api) (2.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.10/dist-packages (from twine->tensorflow-object-detection-api) (7.0.1)\n",
            "Requirement already satisfied: keyring>=15.1 in /usr/lib/python3/dist-packages (from twine->tensorflow-object-detection-api) (23.5.0)\n",
            "Collecting rfc3986>=1.4.0 (from twine->tensorflow-object-detection-api)\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from twine->tensorflow-object-detection-api) (13.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=3.6->twine->tensorflow-object-detection-api) (3.17.0)\n",
            "Collecting nh3>=0.2.14 (from readme-renderer>=35.0->twine->tensorflow-object-detection-api)\n",
            "  Downloading nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine->tensorflow-object-detection-api) (0.18.1)\n",
            "Requirement already satisfied: Pygments>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine->tensorflow-object-detection-api) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->twine->tensorflow-object-detection-api) (3.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (3.0.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (3.0.10)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (3.0.43)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (3.1.3)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (5.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.9.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (1.0.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter->tensorflow-object-detection-api)\n",
            "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (1.3.1)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter->tensorflow-object-detection-api)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->tensorflow-object-detection-api) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->tensorflow-object-detection-api) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->tensorflow-object-detection-api) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->tensorflow-object-detection-api) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->tensorflow-object-detection-api) (4.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->twine->tensorflow-object-detection-api) (0.1.2)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->tensorflow-object-detection-api) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->tensorflow-object-detection-api) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->tensorflow-object-detection-api) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->tensorflow-object-detection-api) (4.19.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->tensorflow-object-detection-api) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter->tensorflow-object-detection-api) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->tensorflow-object-detection-api) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->tensorflow-object-detection-api) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->tensorflow-object-detection-api) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->tensorflow-object-detection-api) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->tensorflow-object-detection-api) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->tensorflow-object-detection-api) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->tensorflow-object-detection-api) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->tensorflow-object-detection-api) (0.18.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->tensorflow-object-detection-api) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->tensorflow-object-detection-api) (1.7.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->tensorflow-object-detection-api) (3.2.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->tensorflow-object-detection-api) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->tensorflow-object-detection-api) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->tensorflow-object-detection-api) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->tensorflow-object-detection-api) (2.21)\n",
            "Building wheels for collected packages: tensorflow-object-detection-api\n",
            "  Building wheel for tensorflow-object-detection-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-object-detection-api: filename=tensorflow_object_detection_api-0.1.1-py3-none-any.whl size=844486 sha256=f3a95a0ef56df30226f34ea069383031bceb8a83ebacece39fb530ed882949af\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/55/68/c084bc2cd93c41fd8f7e2ef9e6bbcb2c35a3e4b49e42044d02\n",
            "Successfully built tensorflow-object-detection-api\n",
            "Installing collected packages: nh3, rfc3986, readme-renderer, qtpy, pkginfo, jedi, requests-toolbelt, twine, qtconsole, jupyter, tensorflow-object-detection-api\n",
            "Successfully installed jedi-0.19.1 jupyter-1.0.0 nh3-0.2.15 pkginfo-1.9.6 qtconsole-5.5.1 qtpy-2.4.1 readme-renderer-43.0 requests-toolbelt-1.0.0 rfc3986-2.0.0 tensorflow-object-detection-api-0.1.1 twine-5.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-object-detection-api\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RFBtEi1jtUTV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daSA_b_EtWoL",
        "outputId": "55e33720-80b6-4a29-ef9e-471c104af526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sm4Xv02tY6x",
        "outputId": "484a5735-934c-4688-ee76-5adc9d08a2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: split-folders[full] in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from split-folders[full]) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install split-folders[full]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fdbYntxtc_Q",
        "outputId": "bf76071e-3285-4252-9d06-1d526cf56f07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying files: 272 files [06:10,  1.36s/ files]\n"
          ]
        }
      ],
      "source": [
        "import splitfolders\n",
        "\n",
        "splitfolders.ratio(\"/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2\",\n",
        "                   output=\"/content/drive/MyDrive/RenamedDataset3/Renamed/split3\",seed=1330, ratio=(.6, .2, .2), group_prefix=None)\n",
        "\n",
        "data_training_path = \"/content/drive/MyDrive/RenamedDataset3/Renamed/split3/train\"\n",
        "data_testing_path = \"/content/drive/MyDrive/RenamedDataset3/Renamed/split3/test\"\n",
        "data_validating_path = \"/content/drive/MyDrive/RenamedDataset3/Renamed/split3/val\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXpwXDAbv0i1",
        "outputId": "21fe8537-8a7a-4d20-f732-fd7c47169368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 162 files belonging to 4 classes.\n",
            "Found 53 files belonging to 4 classes.\n",
            "Found 57 files belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "img_width = 180\n",
        "img_height = 180\n",
        "batch_size = 32\n",
        "\n",
        "data_train = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_training_path,\n",
        "    shuffle=True,\n",
        "    image_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    validation_split=None\n",
        ")\n",
        "\n",
        "data_val = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_validating_path,\n",
        "    image_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    validation_split=None\n",
        ")\n",
        "\n",
        "data_test = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_testing_path,\n",
        "    image_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    validation_split=None\n",
        ")\n",
        "\n",
        "class_names = data_train.class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEiWxPbcwR-R",
        "outputId": "68ca6c2b-a1d8-480c-dfd6-a692fb6b0bfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = MobileNetV2(input_shape=(img_height, img_width, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(len(class_names), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf4u5DdKwcXE",
        "outputId": "717c302b-7df9-465b-f3e7-9a4ace39f4f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "6/6 [==============================] - 16s 2s/step - loss: 1.7626 - accuracy: 0.3086 - val_loss: 1.1231 - val_accuracy: 0.4340\n",
            "Epoch 2/25\n",
            "6/6 [==============================] - 10s 970ms/step - loss: 1.1743 - accuracy: 0.5123 - val_loss: 1.1059 - val_accuracy: 0.4340\n",
            "Epoch 3/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 1.0352 - accuracy: 0.5617 - val_loss: 1.1016 - val_accuracy: 0.5094\n",
            "Epoch 4/25\n",
            "6/6 [==============================] - 10s 979ms/step - loss: 1.0104 - accuracy: 0.5370 - val_loss: 1.0895 - val_accuracy: 0.5283\n",
            "Epoch 5/25\n",
            "6/6 [==============================] - 10s 948ms/step - loss: 0.8660 - accuracy: 0.6358 - val_loss: 1.0460 - val_accuracy: 0.5849\n",
            "Epoch 6/25\n",
            "6/6 [==============================] - 12s 2s/step - loss: 0.8250 - accuracy: 0.6481 - val_loss: 1.0155 - val_accuracy: 0.5849\n",
            "Epoch 7/25\n",
            "6/6 [==============================] - 12s 987ms/step - loss: 0.8020 - accuracy: 0.6481 - val_loss: 0.9999 - val_accuracy: 0.5849\n",
            "Epoch 8/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.7589 - accuracy: 0.6852 - val_loss: 1.1014 - val_accuracy: 0.5472\n",
            "Epoch 9/25\n",
            "6/6 [==============================] - 9s 994ms/step - loss: 0.6255 - accuracy: 0.7531 - val_loss: 1.0880 - val_accuracy: 0.5283\n",
            "Epoch 10/25\n",
            "6/6 [==============================] - 11s 994ms/step - loss: 0.6305 - accuracy: 0.7716 - val_loss: 1.0412 - val_accuracy: 0.5660\n",
            "Epoch 11/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.5731 - accuracy: 0.8086 - val_loss: 1.0994 - val_accuracy: 0.5283\n",
            "Epoch 12/25\n",
            "6/6 [==============================] - 12s 2s/step - loss: 0.5856 - accuracy: 0.7593 - val_loss: 1.1678 - val_accuracy: 0.5472\n",
            "Epoch 13/25\n",
            "6/6 [==============================] - 10s 1s/step - loss: 0.5618 - accuracy: 0.7593 - val_loss: 1.1181 - val_accuracy: 0.5849\n",
            "Epoch 14/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.4986 - accuracy: 0.7840 - val_loss: 1.1382 - val_accuracy: 0.5472\n",
            "Epoch 15/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.4480 - accuracy: 0.8272 - val_loss: 1.2174 - val_accuracy: 0.5472\n",
            "Epoch 16/25\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.4854 - accuracy: 0.7778 - val_loss: 1.1830 - val_accuracy: 0.5849\n",
            "Epoch 17/25\n",
            "6/6 [==============================] - 10s 1s/step - loss: 0.4827 - accuracy: 0.7901 - val_loss: 1.1643 - val_accuracy: 0.5283\n",
            "Epoch 18/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.4981 - accuracy: 0.7716 - val_loss: 1.1967 - val_accuracy: 0.5660\n",
            "Epoch 19/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.4492 - accuracy: 0.8395 - val_loss: 1.1812 - val_accuracy: 0.5660\n",
            "Epoch 20/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.4765 - accuracy: 0.8580 - val_loss: 1.3426 - val_accuracy: 0.5472\n",
            "Epoch 21/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.4712 - accuracy: 0.8148 - val_loss: 1.2861 - val_accuracy: 0.6226\n",
            "Epoch 22/25\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.6521 - accuracy: 0.7284 - val_loss: 1.2199 - val_accuracy: 0.5660\n",
            "Epoch 23/25\n",
            "6/6 [==============================] - 11s 1s/step - loss: 0.4720 - accuracy: 0.8519 - val_loss: 1.3480 - val_accuracy: 0.4906\n",
            "Epoch 24/25\n",
            "6/6 [==============================] - 12s 1s/step - loss: 0.4509 - accuracy: 0.8210 - val_loss: 1.1183 - val_accuracy: 0.5472\n",
            "Epoch 25/25\n",
            "6/6 [==============================] - 10s 1s/step - loss: 0.3966 - accuracy: 0.8765 - val_loss: 1.1129 - val_accuracy: 0.6038\n"
          ]
        }
      ],
      "source": [
        "epochs = 25\n",
        "history = model.fit(data_train, validation_data=data_val, epochs=epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Jh0R1-xBwfo7"
      },
      "outputs": [],
      "source": [
        "model.save('hood_damage_model_mobilenet.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzbtsAiUNzVS",
        "outputId": "76421266-a226-4656-b69a-807ff61a49be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hood_damage_model_mobilenet.pkl']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "import joblib\n",
        "\n",
        "# Assuming hood_damage_model_mobilenet is a trained model\n",
        "hood_damage_model_mobilenet.keras = SVC()\n",
        "# Train or load the model\n",
        "\n",
        "# Now that hood_damage_model_mobilenet is defined and initialized,\n",
        "# you can proceed to save it using joblib.dump()\n",
        "joblib.dump(hood_damage_model_mobilenet, 'hood_damage_model_mobilenet.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO4l2mBEq-Pv",
        "outputId": "de13dda5-0c04-43a0-e10c-dec2c0f1fd01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 2s 477ms/step - loss: 1.9435 - accuracy: 0.4035\n",
            "2/2 [==============================] - 3s 513ms/step\n",
            "Test Loss: 1.9435468912124634\n",
            "Test Accuracy: 0.4035087823867798\n",
            "Precision: 0.64\n",
            "Recall: 0.82051283\n",
            "F1 Score: 0.7191010694868536\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the testing set\n",
        "test_loss, test_accuracy = model.evaluate(data_test)\n",
        "\n",
        "# Calculate additional metrics\n",
        "predictions = model.predict(data_test)\n",
        "y_true = np.concatenate([y for x, y in data_test], axis=0)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "precision = tf.keras.metrics.Precision()\n",
        "precision.update_state(y_true, y_pred)\n",
        "precision_result = precision.result().numpy()\n",
        "\n",
        "recall = tf.keras.metrics.Recall()\n",
        "recall.update_state(y_true, y_pred)\n",
        "recall_result = recall.result().numpy()\n",
        "\n",
        "f1_score = 2 * (precision_result * recall_result) / (precision_result + recall_result)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Precision:\", precision_result)\n",
        "print(\"Recall:\", recall_result)\n",
        "print(\"F1 Score:\", f1_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGLM54b8sLxl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# load the saved model\n",
        "model = tf.keras.models.load_model('/content/hood_damage_model_mobilenet.keras')\n",
        "\n",
        "# define test data directory and image size\n",
        "test_dir = '/content/drive/MyDrive/RenamedDataset3/Renamed/split3/test'\n",
        "img_size = (224, 224)\n",
        "\n",
        "# define batch size and create test data generator\n",
        "batch_size = 32\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(data_test)\n",
        "# print the test accuracy\n",
        "print('Test accuracy:', test_accuracy)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = model.predict(test_data, verbose=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# get true labels of the test data\n",
        "y_true = test_data.classes\n",
        "\n",
        "# generate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "\n",
        "# # make predictions on the test data\n",
        "# y_pred = model.predict(test_data, verbose=1)\n",
        "# y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# # get true labels of the test data\n",
        "# y_true = test_data.classes\n",
        "\n",
        "# # generate confusion matrix\n",
        "# cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# plot confusion matrix as heatmap\n",
        "class_names = test_data.class_indices.keys()\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_cm, annot=True, cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIO3Sg0ItLhv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(data_test)\n",
        "\n",
        "# Get predictions\n",
        "y_pred_prob = model.predict(data_test)\n",
        "y_pred_classes = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Convert data_test labels to numpy array\n",
        "y_true = np.concatenate([y for x, y in data_test], axis=0)\n",
        "\n",
        "# Get class names\n",
        "class_names = data_test.class_names\n",
        "\n",
        "# Calculate metrics\n",
        "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
        "classification_rep = classification_report(y_true, y_pred_classes, target_names=class_names)\n",
        "\n",
        "# Display confusion matrix as an image\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.imshow(confusion_mtx)\n",
        "ax.grid(False)\n",
        "ax.set_xlabel('Predicted labels', fontsize=12, color='black')\n",
        "ax.set_ylabel('True labels', fontsize=12, color='black')\n",
        "ax.set_xticks(np.arange(len(class_names)))\n",
        "ax.set_yticks(np.arange(len(class_names)))\n",
        "ax.set_xticklabels(class_names, fontsize=10, color='black')\n",
        "ax.set_yticklabels(class_names, fontsize=10, color='black')\n",
        "\n",
        "# Add the actual numbers in the cells\n",
        "thresh = confusion_mtx.max() / 2\n",
        "for i in range(len(class_names)):\n",
        "    for j in range(len(class_names)):\n",
        "        ax.text(j, i, format(confusion_mtx[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('\\nClassification Report:')\n",
        "print(classification_rep)\n",
        "print('Accuracy:', accuracy)\n",
        "print('Loss:', loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall charset_normalizer requests\n",
        "!pip install charset_normalizer requests\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "-r3m4gnyyYlp",
        "outputId": "4ec5defd-5f2e-4001-dd4c-8ed8d05ec4f4"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\requests\\compat.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the saved model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhood_damage_model_mobilenet.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\__init__.py:54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible \u001b[38;5;66;03m# line: 65\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\__init__.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible \u001b[38;5;66;03m# line: 65\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\v2\\__init__.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debugging\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster_resolver\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordinator\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollective_all_reduce_strategy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CollectiveAllReduceStrategy \u001b[38;5;28;01mas\u001b[39;00m MultiWorkerMirroredStrategy \u001b[38;5;66;03m# line: 56\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_device_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossDeviceOps \u001b[38;5;66;03m# line: 251\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\experimental\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollective_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _OptionsExported \u001b[38;5;28;01mas\u001b[39;00m CommunicationOptions \u001b[38;5;66;03m# line: 50\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ValueContext \u001b[38;5;66;03m# line: 914\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure_handling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure_handling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreemptionCheckpointHandler \u001b[38;5;66;03m# line: 336\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure_handling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure_handling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TerminationConfig \u001b[38;5;66;03m# line: 74\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure_handling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreemption_watcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreemptionWatcher \u001b[38;5;66;03m# line: 44\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\python\\distribute\\failure_handling\\failure_handling.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_worker_util\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfailure_handling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m failure_handling_util\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\tensorflow\\python\\distribute\\failure_handling\\failure_handling_util.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoves\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01murllib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m request\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\requests\\__init__.py:45\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\requests\\exceptions.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mrequests.exceptions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mThis module contains the set of Requests' exceptions.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m BaseHTTPError\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m CompatJSONDecodeError\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRequestException\u001b[39;00m(\u001b[38;5;167;01mIOError\u001b[39;00m):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    request.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\requests\\compat.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Pythons\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Syntax sugar.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\charset_normalizer\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
            "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\fixmatic\\Lib\\site-packages\\charset_normalizer\\api.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
            "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('hood_damage_model_mobilenet.keras')\n",
        "\n",
        "# Define image dimensions\n",
        "img_width = 180\n",
        "img_height = 180\n",
        "\n",
        "# Function to estimate cost range and decision based on damage severity for hood\n",
        "def estimate_hood_cost(severity):\n",
        "    if severity == \"Minor\":\n",
        "        return \"Repair\", \"$300 - $800\"\n",
        "    elif severity == \"Moderate\":\n",
        "        return \"Repair\", \"$800 - $2000\"\n",
        "    elif severity == \"Severe\":\n",
        "        return \"Replace\", \"Over $2000\"\n",
        "    else:\n",
        "        return \"N/A\", \"N/A\"\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = \"/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/hood (severe)/Hood(severe)_53.png\"\n",
        "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width))\n",
        "image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
        "image_array = tf.expand_dims(image_array, 0)\n",
        "\n",
        "# Predict the class of the image\n",
        "prediction = loaded_model.predict(image_array)\n",
        "predicted_class_index = np.argmax(prediction)\n",
        "confidence = np.max(prediction) * 100\n",
        "\n",
        "# Define class names\n",
        "class_names = ['undamaged', 'minor damage', 'moderate damage', 'severe damage']\n",
        "\n",
        "# Check if the predicted class indicates damage\n",
        "if class_names[predicted_class_index] == 'undamaged' and confidence >= 80:\n",
        "    print(\"The uploaded image shows no damage.\")\n",
        "elif confidence < 20:  # Adjust threshold based on your model's performance\n",
        "    print(\"The uploaded image is invalid or unrelated to vehicle damage.\")\n",
        "else:\n",
        "    # Further analyze the confidence level to categorize the damage\n",
        "    if confidence >= 80:\n",
        "        damage_category = 'Severe'\n",
        "    elif confidence >= 60:\n",
        "        damage_category = 'Moderate'\n",
        "    else:\n",
        "        damage_category = 'Minor'\n",
        "\n",
        "    print(\"The uploaded image shows\", damage_category, \"damage with a confidence of\", confidence, \"%.\")\n",
        "\n",
        "    # Estimate damage cost based on severity\n",
        "    repair_decision, cost_range = estimate_hood_cost(damage_category)\n",
        "\n",
        "    # Display the image with the predicted damage category and cost information\n",
        "   \n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    text = f\"Vehicle Part: Hood\\n\"\n",
        "    text += f\"Predicted Damage: {damage_category}\\n\"\n",
        "    text += f\"Confidence: {confidence:.2f}%\\n\\n\"\n",
        "    text += f\"Repair Decision: {repair_decision}\\n\"\n",
        "    text += f\"Cost Range: {cost_range}\"\n",
        "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnxIyApiytnl"
      },
      "outputs": [],
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# import joblib\n",
        "\n",
        "# app = Flask(__name__)\n",
        "\n",
        "# # Load the machine learning model\n",
        "# model = joblib.load('hood_damage_model_mobilenet.pkl')\n",
        "\n",
        "# @app.route('/predict', methods=['POST'])\n",
        "# def predict():\n",
        "#     data = request.get_json()  # Assuming input data is sent as JSON\n",
        "#     features = data['features']  # Assuming 'features' contains the input data for prediction\n",
        "#     prediction = model.predict(features)\n",
        "#     return jsonify({'prediction': prediction.tolist()})\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Sb8mTbQLvf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPlLn7lFVMCw4f8e8JxP5nB",
      "include_colab_link": true,
      "mount_file_id": "1bjWxME9BwvHe31hZ9JmZJTr4Wd0A_NHl",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
