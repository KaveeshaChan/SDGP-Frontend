{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XzL5MLl1mKnxVtYzOS0O-UitPOPTzoVp",
      "authorship_tag": "ABX9TyPQxGlO9aDfX7gGWFRj1qfS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaveeshaChan/SDGP-SE-37/blob/sandesi/Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WjvL5CPcwsV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d646d076-6a4c-4316-c3f6-1766b589a44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Count for Hood"
      ],
      "metadata": {
        "id": "O3vdbgWwxPKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2\"\n",
        "subdirs = os.listdir(data_dir)\n",
        "\n",
        "for subdir in subdirs:\n",
        "    subdir_path = os.path.join(data_dir, subdir)\n",
        "    if os.path.isdir(subdir_path):\n",
        "        count = len(os.listdir(subdir_path))\n",
        "        print(\"Subfolder:\", subdir, \"Image count:\", count)"
      ],
      "metadata": {
        "id": "4DtDFFaswyce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f68154-5286-4b50-d606-2675b6ffd9e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subfolder: undamaged_hood Image count: 50\n",
            "Subfolder: hood(minor) Image count: 75\n",
            "Subfolder: hood (moderate) Image count: 83\n",
            "Subfolder: hood (severe) Image count: 87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_image(img_path, target_size=(224, 224)):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Normalize the image\n",
        "    return img_array\n"
      ],
      "metadata": {
        "id": "TImvPys80oRi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n"
      ],
      "metadata": {
        "id": "IQsFOvbP1FMJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentation for the image folders Hood(minor)"
      ],
      "metadata": {
        "id": "QE6BlJkAxr-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory of the dataset\n",
        "base_dir = '/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2'\n",
        "\n",
        "# Define the subdirectory of the class to augment\n",
        "subdir = '/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/hood(minor)'\n",
        "\n",
        "# Create an instance of ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    channel_shift_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Generate augmented images until 2000 images per class are reached\n",
        "subdir_path = os.path.join(base_dir, subdir)\n",
        "if os.path.isdir(subdir_path):\n",
        "    num_images = len(os.listdir(subdir_path))\n",
        "    num_augmented_images_needed = 2000 - num_images\n",
        "\n",
        "    # Generate and save additional augmented images\n",
        "    for i in range(num_augmented_images_needed):\n",
        "        # Load a random image from the directory\n",
        "        random_index = np.random.randint(num_images)\n",
        "        img_name = os.listdir(subdir_path)[random_index]\n",
        "        img_path = os.path.join(subdir_path, img_name)\n",
        "        img = np.expand_dims(img_to_array(load_img(img_path)), axis=0)\n",
        "\n",
        "        # Generate and save an augmented version of the image\n",
        "        augmented_img = next(datagen.flow(img, save_to_dir=subdir_path, save_prefix='augmented', save_format='jpg'))\n"
      ],
      "metadata": {
        "id": "-iKA2wd7xXt0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Augmentation for the image folders Hood(moderate)"
      ],
      "metadata": {
        "id": "YDkOiVRL1sJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory of the dataset\n",
        "base_dir = '/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2'\n",
        "\n",
        "# Define the subdirectory of the class to augment\n",
        "subdir = '/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/hood (moderate)'\n",
        "\n",
        "# Create an instance of ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    channel_shift_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Generate augmented images until 2000 images per class are reached\n",
        "subdir_path = os.path.join(base_dir, subdir)\n",
        "if os.path.isdir(subdir_path):\n",
        "    num_images = len(os.listdir(subdir_path))\n",
        "    num_augmented_images_needed = 2000 - num_images\n",
        "\n",
        "    # Generate and save additional augmented images\n",
        "    for i in range(num_augmented_images_needed):\n",
        "        # Load a random image from the directory\n",
        "        random_index = np.random.randint(num_images)\n",
        "        img_name = os.listdir(subdir_path)[random_index]\n",
        "        img_path = os.path.join(subdir_path, img_name)\n",
        "        img = np.expand_dims(img_to_array(load_img(img_path)), axis=0)\n",
        "\n",
        "        # Generate and save an augmented version of the image\n",
        "        augmented_img = next(datagen.flow(img, save_to_dir=subdir_path, save_prefix='augmented', save_format='jpg'))\n"
      ],
      "metadata": {
        "id": "g25GEJzn1-E-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentation for the image folders Hood(Severe)"
      ],
      "metadata": {
        "id": "vjVpIB7K10Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory of the dataset\n",
        "base_dir = '/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2'\n",
        "\n",
        "# Define the subdirectory of the class to augment\n",
        "subdir = '/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/hood (severe)'\n",
        "\n",
        "# Create an instance of ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    channel_shift_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Generate augmented images until 2000 images per class are reached\n",
        "subdir_path = os.path.join(base_dir, subdir)\n",
        "if os.path.isdir(subdir_path):\n",
        "    num_images = len(os.listdir(subdir_path))\n",
        "    num_augmented_images_needed = 2000 - num_images\n",
        "\n",
        "    # Generate and save additional augmented images\n",
        "    for i in range(num_augmented_images_needed):\n",
        "        # Load a random image from the directory\n",
        "        random_index = np.random.randint(num_images)\n",
        "        img_name = os.listdir(subdir_path)[random_index]\n",
        "        img_path = os.path.join(subdir_path, img_name)\n",
        "        img = np.expand_dims(img_to_array(load_img(img_path)), axis=0)\n",
        "\n",
        "        # Generate and save an augmented version of the image\n",
        "        augmented_img = next(datagen.flow(img, save_to_dir=subdir_path, save_prefix='augmented', save_format='jpg'))\n"
      ],
      "metadata": {
        "id": "MYfBmC8i1_sW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentation for the image folders Hood(undamged)"
      ],
      "metadata": {
        "id": "bl_7hRtO13BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory of the dataset\n",
        "base_dir = '/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2'\n",
        "\n",
        "# Define the subdirectory of the class to augment\n",
        "subdir = '/content/drive/MyDrive/RenamedDataset3/Renamed/newDATA2/undamaged_hood'\n",
        "\n",
        "# Create an instance of ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    channel_shift_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Generate augmented images until 2000 images per class are reached\n",
        "subdir_path = os.path.join(base_dir, subdir)\n",
        "if os.path.isdir(subdir_path):\n",
        "    num_images = len(os.listdir(subdir_path))\n",
        "    num_augmented_images_needed = 2000 - num_images\n",
        "\n",
        "    # Generate and save additional augmented images\n",
        "    for i in range(num_augmented_images_needed):\n",
        "        # Load a random image from the directory\n",
        "        random_index = np.random.randint(num_images)\n",
        "        img_name = os.listdir(subdir_path)[random_index]\n",
        "        img_path = os.path.join(subdir_path, img_name)\n",
        "        img = np.expand_dims(img_to_array(load_img(img_path)), axis=0)\n",
        "\n",
        "        # Generate and save an augmented version of the image\n",
        "        augmented_img = next(datagen.flow(img, save_to_dir=subdir_path, save_prefix='augmented', save_format='jpg'))\n"
      ],
      "metadata": {
        "id": "EbCy-zrM15I6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_bI-KSL2dT_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}